{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "KLCNYyxFuPXC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "6NHdYXq9vfAR"
   },
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        result[feature_name] = (df[feature_name] - df[feature_name].mean()) / df[feature_name].std()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "55LM4jWXvgvs"
   },
   "outputs": [],
   "source": [
    "#Importing data\n",
    "data = pd.read_csv('../data/df_final.csv')\n",
    "data = data.drop(columns=['lat', 'lon', 'elv', 'classid', 'c4', 'koeppen_code', 'igbp_land_use'])\n",
    "# Drop AR-Vir and CN-Cng\n",
    "data = data[data.sitename != \"AR-Vir\"]\n",
    "data = data[data.sitename != \"CN-Cng\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "b2MUUMGsxfgr"
   },
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=['plant_functional_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ErzrrgcZviM4",
    "outputId": "09524b34-e03d-43cd-eaaa-b0803ec07f1e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-3bb9fd8c6aed>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sites_df[i]['date'] = pd.to_datetime(sites_df[i]['date'], format=\"%Y-%m-%d\")\n"
     ]
    }
   ],
   "source": [
    "sites = data['sitename'].unique()\n",
    "sites_df = [data[data['sitename'] == site] for site in sites]\n",
    "\n",
    "for i in range(len(sites_df)):\n",
    "    sites_df[i]['date'] = pd.to_datetime(sites_df[i]['date'], format=\"%Y-%m-%d\")\n",
    "    sites_df[i] = sites_df[i].set_index(\"date\")\n",
    "    sites_df[i] = sites_df[i].drop(columns=[\"sitename\"])\n",
    "    sites_df[i] = sites_df[i].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Bmkd3O5Wvj6X"
   },
   "outputs": [],
   "source": [
    "x_time_dep_cols = ['TA_F', 'TA_F_DAY', 'TA_F_NIGHT', 'SW_IN_F', 'LW_IN_F', 'VPD_F', 'PA_F', 'P_F', 'WS_F', 'LE_F_MDS', 'NEE_VUT_REF', 'wscal', 'fpar', 'whc']\n",
    "x_time_invariant_cols = ['plant_functional_type_Cereal crop',\n",
    "       'plant_functional_type_Deciduous Broadleaf Trees',\n",
    "       'plant_functional_type_Evergreen Broadleaf Trees',\n",
    "       'plant_functional_type_Evergreen Needleleaf Trees',\n",
    "       'plant_functional_type_Grass', 'plant_functional_type_Shrub',\n",
    "       'plant_functional_type_Water']\n",
    "x_cols = x_time_dep_cols + x_time_invariant_cols\n",
    "y_col = ['GPP_NT_VUT_REF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "b2J3AFt3vo4G"
   },
   "outputs": [],
   "source": [
    "#define data\n",
    "train_sites = list(range(40))\n",
    "test_sites = [62]\n",
    "\n",
    "train_dfs = [sites_df[i] for i in train_sites]\n",
    "test_dfs = [sites_df[i] for i in test_sites]\n",
    "\n",
    "\n",
    "X_train = [pd.concat([normalize(df[x_time_dep_cols]), df[x_time_invariant_cols]], axis=1).values for df in train_dfs]\n",
    "conditional_train = [torch.tensor(df[x_time_invariant_cols].values, dtype=torch.float32) for df in train_dfs]\n",
    "y_train = [normalize(df[y_col]).values for df in train_dfs]\n",
    "\n",
    "X_test = [pd.concat([normalize(df[x_time_dep_cols]), df[x_time_invariant_cols]], axis=1).values for df in test_dfs]\n",
    "conditional_test = [torch.tensor(df[x_time_invariant_cols].values, dtype=torch.float32) for df in test_dfs]\n",
    "y_test = [normalize(df[y_col]).values for df in test_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "N_gG2e_hvyxJ"
   },
   "outputs": [],
   "source": [
    "# VAE model\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super().__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=input_features, hidden_size=output_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs, (h, c) = self.rnn(x)\n",
    "\n",
    "        return outputs.squeeze(1) #shape=(seq_len, num_dir * output_features)\n",
    "\n",
    "class Reparametrize(nn.Module):\n",
    "    def __init__(self, encoder_output, latent_size):\n",
    "        super().__init__()\n",
    "        self.encoder_output = encoder_output\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.fc_to_mean = nn.Linear(encoder_output, latent_size)\n",
    "        self.fc_to_logvar = nn.Linear(encoder_output, latent_size)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc_to_mean.weight)\n",
    "        nn.init.xavier_uniform_(self.fc_to_logvar.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mean = self.fc_to_mean(x)\n",
    "        self.logvar = self.fc_to_logvar(x)\n",
    "\n",
    "        std = torch.exp(0.5 * self.logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = eps.mul(std).add_(self.mean)\n",
    "        return z, self.mean, self.logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super().__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=input_features, hidden_size=output_features)\n",
    "\n",
    "        self.rnn_to_output = nn.Sequential(\n",
    "            nn.Linear(self.output_features, self.output_features),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.output_features, self.output_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, (h, c) = self.rnn(x)\n",
    "        outputs = outputs.squeeze(1)\n",
    "        return self.rnn_to_output(outputs) #shape=(seq_len, num_dir * output_features)\n",
    "\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super().__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=input_features, hidden_size=output_features)\n",
    "\n",
    "        self.rnn_to_output = nn.Sequential(\n",
    "            nn.Linear(self.output_features, self.output_features),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.output_features, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, (h, c) = self.rnn(x)\n",
    "        outputs = outputs.squeeze(1)\n",
    "        return self.rnn_to_output(outputs) #shape=(seq_len, num_dir * output_features)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, encoder, decoder, reparam):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.reparam = reparam\n",
    "\n",
    "    def forward(self, x, conditional):\n",
    "        x = self.encoder(x) # x has the conditional\n",
    "        z, mean, logvar = self.reparam(x)\n",
    "\n",
    "        # concat the conditoinal to z\n",
    "        z1 = torch.cat([z, conditional], dim=1)\n",
    "        \n",
    "        # decode\n",
    "        x = self.decoder(z1.unsqueeze(1))\n",
    "\n",
    "        return x, mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "iLOAK9fx1Lfd"
   },
   "outputs": [],
   "source": [
    "#loss functions\n",
    "def loss_fn(x_decoded, x, mu, logvar, w):\n",
    "    kl_loss = w * (-0.5) * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    recon_loss = F.mse_loss(x_decoded, x)\n",
    "    return kl_loss + recon_loss, kl_loss, recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "snRYOhoD1JfB"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c63bcfd98eab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mDEVICE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENCODER_INPUT_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mENCODER_OUTPUT_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDECODER_INPUT_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDECODER_OUTPUT_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mreparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReparametrize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREPARAM_INPUT_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mREPARAM_OUTPUT_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNNBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# Resets _flat_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "#define parameter and model (this model should be ran on gpu)\n",
    "ENCODER_INPUT_DIM = len(x_cols)\n",
    "ENCODER_OUTPUT_DIM = 16\n",
    "REPARAM_INPUT_DIM = ENCODER_OUTPUT_DIM\n",
    "LATENT_DIM = 2\n",
    "REPARAM_OUTPUT_DIM = LATENT_DIM\n",
    "DECODER_INPUT_DIM = LATENT_DIM + len(x_time_invariant_cols)\n",
    "DECODER_OUTPUT_DIM = len(x_time_dep_cols)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "encoder = Encoder(ENCODER_INPUT_DIM, ENCODER_OUTPUT_DIM).to(DEVICE)\n",
    "decoder = Decoder(DECODER_INPUT_DIM, DECODER_OUTPUT_DIM).to(DEVICE)\n",
    "reparam = Reparametrize(REPARAM_INPUT_DIM, REPARAM_OUTPUT_DIM).to(DEVICE)\n",
    "model = Model(encoder, decoder, reparam).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dKZsDULn05rZ",
    "outputId": "aded16dc-5fad-4b8c-be52-fa6ed210dc2d"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "EPOCHS = 200\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = 0.0\n",
    "    train_kl_loss = 0.0\n",
    "    train_recon_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    test_kl_loss = 0.0\n",
    "    test_recon_loss = 0.0\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    for (x, c, y) in zip(X_train, conditional_train, y_train):\n",
    "        # Convert to tensors\n",
    "        x = torch.FloatTensor(x).unsqueeze(1).to(DEVICE)\n",
    "        c = torch.FloatTensor(c).to(DEVICE)\n",
    "        y = torch.FloatTensor(y).to(DEVICE)\n",
    "\n",
    "        # Get predictions\n",
    "        out, mean, logvar = model(x, c)\n",
    "\n",
    "        # Remove the conditional from x\n",
    "        x = x.squeeze(1)[:, :len(x_time_dep_cols)]\n",
    "\n",
    "        # Get loss and update\n",
    "        optimizer.zero_grad()\n",
    "        loss, kl_loss, recon_loss = loss_fn(out, x, mean, logvar, 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Update losses\n",
    "        train_loss += loss.item()\n",
    "        train_kl_loss += kl_loss.item()\n",
    "        train_recon_loss += recon_loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (x, c, y) in zip(X_test, conditional_test, y_test):\n",
    "            # Convert to tensors\n",
    "            x = torch.FloatTensor(x).unsqueeze(1).to(DEVICE)\n",
    "            c = torch.FloatTensor(c).to(DEVICE)\n",
    "            y = torch.FloatTensor(y).to(DEVICE)\n",
    "\n",
    "            # Get predictions\n",
    "            out, mean, logvar = model(x, c)\n",
    "\n",
    "            # Remove the conditional from x\n",
    "            x = x.squeeze(1)[:, :len(x_time_dep_cols)]\n",
    "\n",
    "            # Get loss and update\n",
    "            loss, kl_loss, recon_loss = loss_fn(out, x, mean, logvar, 1)\n",
    "        \n",
    "            # Update losses\n",
    "            test_loss += loss.item()\n",
    "            test_kl_loss += kl_loss.item()\n",
    "            test_recon_loss += recon_loss.item()\n",
    "\n",
    "    \n",
    "\n",
    "    train_loss /= len(X_train)\n",
    "    train_kl_loss /= len(X_train)\n",
    "    train_recon_loss /= len(X_train)\n",
    "    test_loss /= len(X_test)\n",
    "    test_kl_loss /= len(X_test)\n",
    "    test_recon_loss /= len(X_test)\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Train loss: {train_loss:.4f} | Recon loss: {train_recon_loss:.4f} | KL loss: {train_kl_loss:.4f}\")\n",
    "    print(f\"Test loss: {test_loss:.4f} | Recon loss: {test_recon_loss:.4f} | KL loss: {test_kl_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXCHBwk31GDj"
   },
   "outputs": [],
   "source": [
    "# Visualization of the result \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def visualize_recon(model, x, c, col):\n",
    "    out, mean, logvar = model(x, c)\n",
    "    out = out.detach().cpu().numpy()\n",
    "    mean = mean.detach().cpu().numpy()\n",
    "    logvar = logvar.detach().cpu().numpy()\n",
    "    x = x.detach().cpu().numpy()\n",
    "    c = c.detach().cpu().numpy()\n",
    "\n",
    "    x = x.squeeze(1)[:, :len(x_time_dep_cols)]\n",
    "\n",
    "    x_axis = range(x.shape[0])\n",
    "    plt.figure(dpi=200)\n",
    "    plt.scatter(x=x_axis, y=x[:, col], label='GT', s=1)\n",
    "    plt.scatter(x=x_axis, y=out[:, col], label='Preds', s=1)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_latent_1(model, x, c, months, columns):\n",
    "    out, mean, logvar = model(x, c)\n",
    "    mean = mean.detach().cpu().numpy()\n",
    "\n",
    "    # pca = PCA(n_components=2)\n",
    "    # mean = pca.fit_transform(mean)\n",
    "    plt.figure(dpi=200)\n",
    "\n",
    "    summer_months = np.argwhere((months == 6) | (months == 7) | (months == 8)).flatten()\n",
    "    winter_months = np.argwhere((months == 1) | (months == 2) | (months == 12)).flatten()\n",
    "    spring_months = np.argwhere((months == 3) | (months == 4) | (months == 5)).flatten()\n",
    "    fall_months = np.argwhere((months == 9) | (months == 10) | (months == 11)).flatten()\n",
    "    \n",
    "    summer_mean = mean[summer_months, :]\n",
    "    winter_mean = mean[winter_months, :]\n",
    "    spring_mean = mean[spring_months, :]\n",
    "    fall_mean = mean[fall_months, :]\n",
    "    plt.scatter(x=summer_mean[:, 0], y=summer_mean[:, 1], s=1, c='red')\n",
    "    plt.scatter(x=winter_mean[:, 0], y=winter_mean[:, 1], s=1, c='blue')\n",
    "    plt.scatter(x=spring_mean[:, 0], y=spring_mean[:, 1], s=1, c='yellow')\n",
    "    plt.scatter(x=fall_mean[:, 0], y=fall_mean[:, 1], s=1, c='orange')\n",
    "    \n",
    "    plt.title(\"Latent space\")\n",
    "    plt.show()\n",
    "\n",
    "def visualize_latent_2(model, x, c, y, columns):\n",
    "    out, mean, logvar = model(x, c)\n",
    "    mean = mean.detach().cpu().numpy()\n",
    "\n",
    "    # pca = PCA(n_components=2)\n",
    "    # mean = pca.fit_transform(mean)\n",
    "    plt.figure(dpi=200)\n",
    "    plt.scatter(x=mean[:, 0], y=mean[:, 1], s=1, c=y)\n",
    "    \n",
    "    plt.title(\"Latent space\")\n",
    "    plt.show()\n",
    "\n",
    "def visualize_gpp(model, x, c, y):\n",
    "    out, mean, logvar = model(x, c)\n",
    "    out = out.detach().cpu().numpy()\n",
    "    x_axis = range(out.shape[0])\n",
    "    y = y.detach().cpu().numpy()\n",
    "\n",
    "    plt.figure(dpi=200)\n",
    "    plt.scatter(x=x_axis, y=out, s=1, label=\"Preds\")\n",
    "    plt.scatter(x=x_axis, y=y, s=1, label=\"GT\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfgM38XV3Heo"
   },
   "outputs": [],
   "source": [
    "sites_with_dates_df = [data[data['sitename'] == site] for site in sites]\n",
    "\n",
    "def convert_to_months(values):\n",
    "    mapping = {'01': 'Jan', '02': 'Feb', '03': 'Mar', '04': 'Apr', '05': 'May', '06': 'Jun', '07': 'Jul', '08': 'Aug', '09': 'Sep', '10': 'Oct', '11': 'Nov', '12':'Dec'}\n",
    "    mapping = {'01': 1, '02': 2, '03': 3, '04': 4, '05': 5, '06': 6, '07': 7, '08': 8, '09': 9, '10': 10, '11': 11, '12': 12}\n",
    "    months = [mapping[v.split('-')[1]] for v in values]\n",
    "    return np.array(months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_XhbxyL81Ni",
    "outputId": "c7c2335f-f521-4137-ad01-d1ebcff54e49"
   },
   "outputs": [],
   "source": [
    "np.argwhere(sites=='IT-Tor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "id": "enGOP3Hc6bVh",
    "outputId": "1cc57f8d-6465-4a16-e776-b8347834c99c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_to_months' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f649136e5599>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m41\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmonths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_months\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msites_with_dates_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_to_months' is not defined"
     ]
    }
   ],
   "source": [
    "# output plot for paper(Figure 7)\n",
    "site = 41\n",
    "col = 0\n",
    "months = convert_to_months(sites_with_dates_df[site].date.values)\n",
    "\n",
    "x = torch.FloatTensor(X_train[site]).unsqueeze(1).to(DEVICE)\n",
    "c = torch.FloatTensor(conditional_train[site]).to(DEVICE)\n",
    "y = torch.FloatTensor(y_train[site]).to(DEVICE)\n",
    "# visualize_recon(model, x, c, col)\n",
    "# visualize_latent_1(model, x, c, months, x_cols)\n",
    "visualize_latent_1(model, x, c, months, x_cols)\n",
    "visualize_latent_2(model, x, c, y_train[site], x_cols)\n",
    "# visualize_gpp(model, x, c, y)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FLUXNET_Fixed_VAE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
